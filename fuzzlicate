#!/usr/bin/env python
import argparse
import json
import hashlib
import os
import sys
import io
from multiprocessing.dummy import Pool as ThreadPool

try:
    import xxhash
    HAS_XXHASH = True
except ImportError:
    print 'consider installing xxhash on your system to speed up indexing.'
    HAS_XXHASH = False

def checksum(filepath):
    """
    Produce a checksum for a file
    """
    if HAS_XXHASH:
        # hash_ = hashlib.sha256()
        hash_ = xxhash.xxh64()
    else:
        hash_ = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            hash_.update(chunk)
    return hash_.hexdigest()


class DupeFinder(object):
    def __init__(self):
        self.theirs = {}
        self.ours = {}
        self.output_name = 'ours.json'

    def dump_ours(self):
        json_string = unicode(json.dumps(self.ours, ensure_ascii=False))

        with io.open(self.output_name, 'w', encoding='utf-8') as f:
            f.write(json_string)

    def compare(self):
        for fhash, filenames in self.ours.iteritems():
            if fhash in self.theirs:
                yield {'ours': filenames, 'theirs': self.theirs[fhash]}

    def inspect(self, rootpath):
        filedb = {}
        filelist = []
        done = []

        def add_hash(fullpath):
            # print fullpath
            done.append(fullpath)
            if len(done)%100 == 99:
                print '{}% done'.format(round(float(len(done))/len(filelist), 3)*100)
            # return
            fhash = unicode(checksum(fullpath), 'utf-8')
            if fhash not in filedb:
                filedb[fhash] = [fullpath]
            else:
                filedb[fhash].append(fullpath)
            
        #build list
        for root, dirs, files in os.walk(rootpath):
            for file_ in files:
                #fullpath = os.path.join(root, file_)
                try:
                    fullpath = unicode(os.path.join(root, file_), 'utf-8')
                except Exception as err:
                    print '{} in {} can not be converted to unicode.'.format(file_, root)
                    continue

                if not os.path.isfile(fullpath):
                    print '[WARN] Not a file: {}'.format(fullpath)
                    continue

                filelist.append(fullpath)


        pool = ThreadPool(6)
        pool.map(add_hash, filelist)

        return filedb



def main():
    # Parse the arguments //////////////////////////////////////////////////////////
    parser = argparse.ArgumentParser(prog='SUBCOMMAND')
    parser.add_argument('--root-dir', type=str, help='Root dir to traverse')
    parser.add_argument('--our-db', type=str)
    parser.add_argument('--their-db', type=str)
    parser.add_argument('--delete', action='store_true', help='Delete duplicates when comparing to THEIRS', default=False)
    parser.add_argument('--dontcare', action='store_true', help='Do not confirm on deletion', default=False)
    parser.add_argument('--dump', action='store_true', help='dump list', default=False)

    if len(sys.argv) == 1:
        parser.print_help()
        return 1

    args = parser.parse_args()

    # Handle the arguments /////////////////////////////////////////////////////////
    df = DupeFinder()
    if args.root_dir is None and args.our_db is None:
        print 'You need to specify either --root-dir <dir> or --our-db <dbfile>'
        return 1

    if args.root_dir is not None:
        local_root = args.root_dir
        if not os.path.isdir(local_root):
            print 'Not a dir:', local_root
            return
        df.ours = df.inspect(local_root)

    if args.our_db is not None:
        if not os.path.isfile(args.our_db):
            print 'No such file:', args.our_db
            return
        with open(args.our_db) as logfile:
            df.ours = json.load(logfile)

    if args.their_db is not None:
        if not os.path.isfile(args.their_db):
            print 'No such file:', args.their_db
            return
        with open(args.their_db) as logfile:
            df.theirs = json.load(logfile)

        print 'comparing ours against theirs:'
        for dupe_dict in df.compare():
            print '\n--------------------------------------------------'
            for k, filenames in dupe_dict.iteritems():
                print k, '/'.join(filenames)

            if args.their_db is not None:
                if args.delete:
                    print 'D', ''.join(dupe_dict['ours']), '???'
                    if not args.dontcare:
                        answer = raw_input('[y/N]\n')
                        if answer == 'y' or args.dontcare:
                            for f in dupe_dict['ours']:
                                os.unlink(f)
                    else:
                        for f in dupe_dict['ours']:
                            os.unlink(f)
        return

    dupes = [(f, df.ours[f]) for f in df.ours.keys() if len(df.ours[f]) > 1]
    if dupes:
        print '---'
        for fhash, filenames in dupes:
            print '['
            for f in filenames:
                print '\t', f
            print ']'
    
    else:
        print 'No duplicates'

    if args.dump:
        df.dump_ours()




if __name__ == '__main__':
    main()